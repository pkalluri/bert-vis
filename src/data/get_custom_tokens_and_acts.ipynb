{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb5f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b21a5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/john1/scr1/baom/text'\n",
    "wiki_dirs = [f'{root_dir}/AA', f'{root_dir}/AB', f'{root_dir}/AC']\n",
    "\n",
    "search_tokens = ['Muslim']\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "device = -1 # set to -1 if not using GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194fd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(wiki_dirs):\n",
    "    hits = []\n",
    "    num_sents = 0\n",
    "\n",
    "    for wiki_dir in wiki_dirs:\n",
    "        for subdir, dirs, files in os.walk(wiki_dir):\n",
    "            for f in files:\n",
    "                wiki_text = os.path.join(subdir, f)\n",
    "                with open(wiki_text, \"r\") as wiki_file:\n",
    "                    for article in wiki_file.readlines():\n",
    "                        wiki_file = json.loads(article)\n",
    "                        title = wiki_file['title']\n",
    "                        text = wiki_file['text']\n",
    "                        \n",
    "                        contained_tokens = []\n",
    "                        for i in search_tokens:\n",
    "                            if i not in text:\n",
    "                                continue\n",
    "                            else:\n",
    "                                contained_tokens.append(i)\n",
    "                        \n",
    "                        if not contained_tokens:\n",
    "                            continue\n",
    "                        \n",
    "                        sentences = sent_tokenize(text)\n",
    "                        num_sents += len(sentences)\n",
    "\n",
    "                        for i, sent in enumerate(sentences):\n",
    "                            toks = []\n",
    "                            for tok in contained_tokens:\n",
    "                                if tok in sent:\n",
    "                                    toks.append(tok)\n",
    "                            if toks:\n",
    "    #                             data = {\"title\": title, \"tokens\": ','.join(toks), \"sentence\": sent, \"sentence_idx\": i, \"path\": wiki_text}\n",
    "                                data = {\"title\": title, \"sentence\": sent, \"sentence_idx\": i, \"path\": wiki_text, \"toks\":\",\".join(toks)}\n",
    "                                hits.append(data)\n",
    "\n",
    "                            if len(hits) == num_samples:\n",
    "                                return hits, num_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63822435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searched thru 599 sentences\n"
     ]
    }
   ],
   "source": [
    "hits, num_sents = get_samples(wiki_dirs)\n",
    "\n",
    "print(f'searched thru {num_sents} sentences')\n",
    "\n",
    "df = pd.DataFrame(hits)\n",
    "# df.to_csv('/john1/scr1/baom/gender_race_in_wiki.tsv', sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50087fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_41f00d8c_10e5_11ec_ab92_150a34f023adrow0_col1 {\n",
       "            width-min:  300px;\n",
       "        }    #T_41f00d8c_10e5_11ec_ab92_150a34f023adrow1_col1 {\n",
       "            width-min:  300px;\n",
       "        }    #T_41f00d8c_10e5_11ec_ab92_150a34f023adrow2_col1 {\n",
       "            width-min:  300px;\n",
       "        }    #T_41f00d8c_10e5_11ec_ab92_150a34f023adrow3_col1 {\n",
       "            width-min:  300px;\n",
       "        }    #T_41f00d8c_10e5_11ec_ab92_150a34f023adrow4_col1 {\n",
       "            width-min:  300px;\n",
       "        }    #T_41f00d8c_10e5_11ec_ab92_150a34f023adrow5_col1 {\n",
       "            width-min:  300px;\n",
       "        }    #T_41f00d8c_10e5_11ec_ab92_150a34f023adrow6_col1 {\n",
       "            width-min:  300px;\n",
       "        }    #T_41f00d8c_10e5_11ec_ab92_150a34f023adrow7_col1 {\n",
       "            width-min:  300px;\n",
       "        }    #T_41f00d8c_10e5_11ec_ab92_150a34f023adrow8_col1 {\n",
       "            width-min:  300px;\n",
       "        }    #T_41f00d8c_10e5_11ec_ab92_150a34f023adrow9_col1 {\n",
       "            width-min:  300px;\n",
       "        }</style><table id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023ad\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >title</th>        <th class=\"col_heading level0 col1\" >sentence</th>        <th class=\"col_heading level0 col2\" >sentence_idx</th>        <th class=\"col_heading level0 col3\" >path</th>        <th class=\"col_heading level0 col4\" >toks</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adlevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow0_col0\" class=\"data row0 col0\" >Pravin Togadia</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow0_col1\" class=\"data row0 col1\" >In January 2002, he asked Hindus to cut all relations with Muslims.</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow0_col2\" class=\"data row0 col2\" >19</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow0_col3\" class=\"data row0 col3\" >/john1/scr1/baom/text/AA/wiki_56</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow0_col4\" class=\"data row0 col4\" >Muslim</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adlevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow1_col0\" class=\"data row1 col0\" >Pravin Togadia</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow1_col1\" class=\"data row1 col1\" >Togadia in turn ridiculed Modi's efforts to reach out to Muslims through his \"sadbhavana\" initiatives.</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow1_col2\" class=\"data row1 col2\" >31</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow1_col3\" class=\"data row1 col3\" >/john1/scr1/baom/text/AA/wiki_56</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow1_col4\" class=\"data row1 col4\" >Muslim</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adlevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow2_col0\" class=\"data row2 col0\" >Pravin Togadia</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow2_col1\" class=\"data row2 col1\" >In April 2014, a First Information Report was registered against Togadia in Bhavnagar after an alleged hate speech instructing Hindus to evict Muslims from their neighbourhoods.</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow2_col2\" class=\"data row2 col2\" >50</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow2_col3\" class=\"data row2 col3\" >/john1/scr1/baom/text/AA/wiki_56</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow2_col4\" class=\"data row2 col4\" >Muslim</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adlevel0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow3_col0\" class=\"data row3 col0\" >Siberian Tatars</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow3_col1\" class=\"data row3 col1\" >The term Siberian Tatar covers three autochthonous groups, all Sunni Muslims of the Hanafi madhab, found in southern Siberia.</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow3_col2\" class=\"data row3 col2\" >23</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow3_col3\" class=\"data row3 col3\" >/john1/scr1/baom/text/AA/wiki_56</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow3_col4\" class=\"data row3 col4\" >Muslim</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adlevel0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow4_col0\" class=\"data row4 col0\" >Siberian Tatars</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow4_col1\" class=\"data row4 col1\" >Since the penetration of Islam until the 1920s after the Russian Revolution, Siberian Tatars, like all Muslim nations, were using an alphabet that had been based on Arabic script.</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow4_col2\" class=\"data row4 col2\" >33</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow4_col3\" class=\"data row4 col3\" >/john1/scr1/baom/text/AA/wiki_56</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow4_col4\" class=\"data row4 col4\" >Muslim</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adlevel0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow5_col0\" class=\"data row5 col0\" >Women in Iran</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow5_col1\" class=\"data row5 col1\" >They, in turn, handed it to the Byzantines, from whom the Arab conquerors turned it into the hijab, transmitting it over the vast reaches of the Muslim world.</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow5_col2\" class=\"data row5 col2\" >32</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow5_col3\" class=\"data row5 col3\" >/john1/scr1/baom/text/AA/wiki_56</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow5_col4\" class=\"data row5 col4\" >Muslim</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adlevel0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow6_col0\" class=\"data row6 col0\" >Women in Iran</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow6_col1\" class=\"data row6 col1\" >Fatimah inspired her husband as a devout Muslim.</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow6_col2\" class=\"data row6 col2\" >109</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow6_col3\" class=\"data row6 col3\" >/john1/scr1/baom/text/AA/wiki_56</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow6_col4\" class=\"data row6 col4\" >Muslim</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adlevel0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow7_col0\" class=\"data row7 col0\" >Women in Iran</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow7_col1\" class=\"data row7 col1\" >Later, after the Muslim Arabs conquered Sassanid Iran, early Muslims adopted veiling as a result of their exposure to the strong Iranian cultural influence.</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow7_col2\" class=\"data row7 col2\" >260</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow7_col3\" class=\"data row7 col3\" >/john1/scr1/baom/text/AA/wiki_56</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow7_col4\" class=\"data row7 col4\" >Muslim</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adlevel0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow8_col0\" class=\"data row8 col0\" >Lise Payette</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow8_col1\" class=\"data row8 col1\" >The proposal, seen to target Muslim women, was widely criticized even by some Quebec nationalists.</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow8_col2\" class=\"data row8 col2\" >45</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow8_col3\" class=\"data row8 col3\" >/john1/scr1/baom/text/AA/wiki_56</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow8_col4\" class=\"data row8 col4\" >Muslim</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adlevel0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow9_col0\" class=\"data row9 col0\" >Bhavani, Tamil Nadu</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow9_col1\" class=\"data row9 col1\" >As per the religious census of 2011, Bhavani had 93.33% Hindus, 4.24% Muslims, 2.35% Christians, 0.01% Sikhs, 0.05% following other religions and 0.02% following no religion or did not indicate any religious preference.</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow9_col2\" class=\"data row9 col2\" >71</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow9_col3\" class=\"data row9 col3\" >/john1/scr1/baom/text/AA/wiki_56</td>\n",
       "                        <td id=\"T_41f00d8c_10e5_11ec_ab92_150a34f023adrow9_col4\" class=\"data row9 col4\" >Muslim</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9217db21d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.style.set_properties(subset=['sentence'], **{'width-min': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc32c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Given any data directory containing a doc.txt file, uses BERT or GPT to generate\n",
    "a corresponding tokens.pickle file and a corresponding activations.npz file.\"\"\"\n",
    "from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import ast\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.insert(0, os.path.abspath('.'))  # add CWD to path\n",
    "\n",
    "max_docs = None  # Max number of documents to read or None. If None, this is ignored.\n",
    "max_contexts = None  # Max number of contexts to read or None. If None, this is ignored.\n",
    "max_toks = 30  # Max number of tokens in an acceptable document. If None, this is ignored.\n",
    "\n",
    "model_type = 'bert' # 'gpt'\n",
    "\n",
    "random_state = 1\n",
    "frac = 1.0 # 0.02 // fraction of rows to sample from in provided .tsv file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f2fd01c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2195ff5f5f482aadf69b34e08bb1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0: (40 tokens) --> 40 total contexts\n",
      "1\n",
      "Doc 1: (62 tokens) --> 102 total contexts\n",
      "2\n",
      "Doc 2: (14 tokens) --> 116 total contexts\n",
      "3\n",
      "Doc 3: (36 tokens) --> 152 total contexts\n",
      "4\n",
      "Doc 4: (16 tokens) --> 168 total contexts\n",
      "5\n",
      "Doc 5: (31 tokens) --> 199 total contexts\n",
      "6\n",
      "Doc 6: (30 tokens) --> 229 total contexts\n",
      "7\n",
      "Doc 7: (33 tokens) --> 262 total contexts\n",
      "8\n",
      "Doc 8: (20 tokens) --> 282 total contexts\n",
      "9\n",
      "Doc 9: (39 tokens) --> 321 total contexts\n",
      "10\n",
      "\n",
      "Found 10 docs & 321 contexts and obtained activations of shape (321, 768)\n",
      "Ignored 0 docs longer than 30 tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "if model_type == 'bert':\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    model = BertModel.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
    "elif model_type == 'gpt':\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)\n",
    "else:\n",
    "    print(\"Incorrect model_type set.\")\n",
    "    exit()\n",
    "\n",
    "if device != -1:\n",
    "    # move the model to the GPU\n",
    "    torch.cuda.set_device(device)\n",
    "    device = torch.device(\"cuda\", device)\n",
    "    model.to(device)\n",
    "\n",
    "df = df[df['sentence'].map(len) < 512]\n",
    "df_sub = df.sample(frac = frac, random_state = random_state)\n",
    "\n",
    "if 'tokens' in df_sub.columns:\n",
    "    df_sub.drop(columns=['tokens'], inplace=True)\n",
    "\n",
    "# Create a list of contexts. Each context will be a tuple: (doc's tokens, position in doc).\n",
    "contexts = []\n",
    "# Create a dictionary to map layer to list of docs' activations.\n",
    "# Each doc's activations will be size (# contexts x size of embedding)\n",
    "layers = {}\n",
    "n_docs_consolidated = 0\n",
    "n_long_docs = 0\n",
    "\n",
    "for _, row in tqdm(df_sub.iterrows()):\n",
    "\n",
    "    sent = row['sentence']\n",
    "    inputs = tokenizer(sent, return_tensors=\"pt\")\n",
    "    tokens = [tokenizer.decode(i).replace(' ', '') for i in inputs['input_ids'].tolist()[0]]\n",
    "\n",
    "    try:\n",
    "        outputs = model(**inputs)\n",
    "        hidden_state = outputs.hidden_states\n",
    "        hidden_state = torch.stack(hidden_state, dim=0)\n",
    "        hidden_state = torch.squeeze(hidden_state, dim=1)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(row['sentence'])\n",
    "        hidden_state = ()\n",
    "\n",
    "    for tok_i in range(len(tokens)):\n",
    "        context = (tokens, tok_i)\n",
    "        contexts.append(context)\n",
    "\n",
    "    num_layers = hidden_state.shape[0]\n",
    "    for l in range(num_layers):\n",
    "        layer = f'arr_{l}'\n",
    "\n",
    "        if layer not in layers:\n",
    "            layers[layer] = hidden_state[l, :, :,].detach().numpy()\n",
    "        else:\n",
    "            layers[layer] = np.concatenate([layers[layer], hidden_state[l, :, :,].detach().numpy()])\n",
    "\n",
    "    print(f'Doc {n_docs_consolidated}: ({len(tokens)} tokens) --> {len(contexts)} total contexts')\n",
    "    n_docs_consolidated += 1\n",
    "    print(n_docs_consolidated)\n",
    "    if n_docs_consolidated == max_docs:\n",
    "        break  # Done\n",
    "\n",
    "print(f'Found {n_docs_consolidated} docs & {len(contexts)} contexts and obtained activations of shape {layers[layer].shape}')\n",
    "if max_toks:\n",
    "    print(f'Ignored {n_long_docs} docs longer than {max_toks} tokens.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9b467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
