{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e708501",
   "metadata": {},
   "source": [
    "## Get sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62df293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a34f3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/john1/scr1/baom/text'\n",
    "wiki_dirs = [f'{root_dir}/AA', f'{root_dir}/AB', f'{root_dir}/AC']\n",
    "\n",
    "search_tokens = ['Muslim']\n",
    "num_samples = 100 # number of samples to return\n",
    "device = -1 # set to -1 if not using GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e50f69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(wiki_dirs):\n",
    "    hits = []\n",
    "    num_sents = 0\n",
    "\n",
    "    for wiki_dir in wiki_dirs:\n",
    "        for subdir, dirs, files in os.walk(wiki_dir):\n",
    "            for f in files:\n",
    "                wiki_text = os.path.join(subdir, f)\n",
    "                with open(wiki_text, \"r\") as wiki_file:\n",
    "                    for article in wiki_file.readlines():\n",
    "                        wiki_file = json.loads(article)\n",
    "                        title = wiki_file['title']\n",
    "                        text = wiki_file['text']\n",
    "                        \n",
    "                        contained_tokens = []\n",
    "                        for i in search_tokens:\n",
    "                            if i not in text:\n",
    "                                continue\n",
    "                            else:\n",
    "                                contained_tokens.append(i)\n",
    "                        if not contained_tokens:\n",
    "                            continue\n",
    "                        \n",
    "                        sentences = sent_tokenize(text)\n",
    "                        num_sents += len(sentences)\n",
    "\n",
    "                        for i, sent in enumerate(sentences):\n",
    "                            toks = []\n",
    "                            for tok in contained_tokens:\n",
    "                                if tok in sent:\n",
    "                                    toks.append(tok)\n",
    "                            if toks:\n",
    "    #                             data = {\"title\": title, \"tokens\": ','.join(toks), \"sentence\": sent, \"sentence_idx\": i, \"path\": wiki_text}\n",
    "                                data = {\"title\": title, \"sentence\": sent, \"sentence_idx\": i, \"path\": wiki_text, \"toks\":\",\".join(toks)}\n",
    "                                hits.append(data)\n",
    "\n",
    "                            if len(hits) == num_samples:\n",
    "                                return hits, num_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae5bd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searched thru 599 sentences\n"
     ]
    }
   ],
   "source": [
    "hits, num_sents = get_samples(wiki_dirs)\n",
    "print(f'searched thru {num_sents} sentences')\n",
    "df = pd.DataFrame(hits)\n",
    "df.style.set_properties(subset=['sentence'], **{'width-min': '300px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22307c5",
   "metadata": {},
   "source": [
    "## Get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79693f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Given any data directory containing a doc.txt file, uses BERT or GPT to generate\n",
    "a corresponding tokens.pickle file and a corresponding activations.npz file.\"\"\"\n",
    "from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import ast\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.insert(0, os.path.abspath('.'))  # add CWD to path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b36c06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_docs = None  # Max number of documents to read or None. If None, this is ignored.\n",
    "max_contexts = None  # Max number of contexts to read or None. If None, this is ignored.\n",
    "max_toks = 30  # Max number of tokens in an acceptable document. If None, this is ignored.\n",
    "\n",
    "model_type = 'bert' # 'gpt'\n",
    "\n",
    "random_state = 1\n",
    "frac = 1.0 # 0.02 // fraction of rows to sample from in provided .tsv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c93b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "if model_type == 'bert':\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    model = BertModel.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
    "elif model_type == 'gpt':\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)\n",
    "else:\n",
    "    print(\"Incorrect model_type set.\")\n",
    "    exit()\n",
    "\n",
    "if device != -1:\n",
    "    # move the model to the GPU\n",
    "    torch.cuda.set_device(device)\n",
    "    device = torch.device(\"cuda\", device)\n",
    "    model.to(device)\n",
    "\n",
    "df = df[df['sentence'].map(len) < 512]\n",
    "df_sub = df.sample(frac = frac, random_state = random_state)\n",
    "\n",
    "if 'tokens' in df_sub.columns:\n",
    "    df_sub.drop(columns=['tokens'], inplace=True)\n",
    "\n",
    "# Create a list of contexts. Each context will be a tuple: (doc's tokens, position in doc).\n",
    "contexts = []\n",
    "# Create a dictionary to map layer to list of docs' activations.\n",
    "# Each doc's activations will be size (# contexts x size of embedding)\n",
    "layers = {}\n",
    "n_docs_consolidated = 0\n",
    "n_long_docs = 0\n",
    "\n",
    "for _, row in tqdm(df_sub.iterrows()):\n",
    "\n",
    "    sent = row['sentence']\n",
    "    inputs = tokenizer(sent, return_tensors=\"pt\")\n",
    "    tokens = [tokenizer.decode(i).replace(' ', '') for i in inputs['input_ids'].tolist()[0]]\n",
    "\n",
    "    try:\n",
    "        outputs = model(**inputs)\n",
    "        hidden_state = outputs.hidden_states\n",
    "        hidden_state = torch.stack(hidden_state, dim=0)\n",
    "        hidden_state = torch.squeeze(hidden_state, dim=1)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(row['sentence'])\n",
    "        hidden_state = ()\n",
    "\n",
    "    for tok_i in range(len(tokens)):\n",
    "        context = (tokens, tok_i)\n",
    "        contexts.append(context)\n",
    "\n",
    "    num_layers = hidden_state.shape[0]\n",
    "    for l in range(num_layers):\n",
    "        layer = f'arr_{l}'\n",
    "\n",
    "        if layer not in layers:\n",
    "            layers[layer] = hidden_state[l, :, :,].detach().numpy()\n",
    "        else:\n",
    "            layers[layer] = np.concatenate([layers[layer], hidden_state[l, :, :,].detach().numpy()])\n",
    "\n",
    "    print(f'Doc {n_docs_consolidated}: ({len(tokens)} tokens) --> {len(contexts)} total contexts')\n",
    "    n_docs_consolidated += 1\n",
    "    print(n_docs_consolidated)\n",
    "    if n_docs_consolidated == max_docs:\n",
    "        break  # Done\n",
    "\n",
    "print(f'Found {n_docs_consolidated} docs & {len(contexts)} contexts and obtained activations of shape {layers[layer].shape}')\n",
    "if max_toks:\n",
    "    print(f'Ignored {n_long_docs} docs longer than {max_toks} tokens.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
