{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88890d77-6183-4bc0-92dd-66785d26a2ce",
   "metadata": {},
   "source": [
    "# Analyze my region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc5125-3886-48f4-9733-d7de86819dcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82309f28-7b63-45ba-b796-7d23c32465b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Active notebook.')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../..'))  # distances --> vis-wiki --> analysis-and-vis --> src\n",
    "# This form of import is reasonable research practice because many directories may want to use the same utils,\n",
    "# but note that this is a bad practice for publishing packages because directories should be modular, with all utils inside them.\n",
    "# Consider changing before publicly publishing code.\n",
    "# load\n",
    "from utils import references as refs\n",
    "import pickle\n",
    "import numpy as np\n",
    "from utils.Token import Token\n",
    "from utils.misc_util import select_layers\n",
    "from collections import Counter\n",
    "# process\n",
    "import random\n",
    "random.seed(0)\n",
    "from utils.ModelType import ModelType, get_generic, berts, gpts\n",
    "from utils.MyModel import MyModel\n",
    "# calculate distances\n",
    "from utils.acts_util import get_angles, get_euclidean_distances, spherize\n",
    "import pandas as pd\n",
    "from utils.FastNearestNeighbors import FastNearestNeighbors\n",
    "# vis\n",
    "import plotly.express as px\n",
    "from plotly.colors import qualitative as color_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48780c37-f6d2-448d-9765-18b73efeb30c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e9cc9f-121f-43e6-a638-a631db38c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU MUST SET THESE TO YOUR PATHS TO POINT TO YOUR DATA LOCATIONS\n",
    "model_type = ModelType.bert_base_cased\n",
    "dataset_dir = f\"/atlas/u/pkalluri/bert-vis/big-data/{get_generic(model_type)}/{model_type.value}/\"\n",
    "glove_file = f\"/atlas/u/pkalluri/bert-vis/big-data/glove/glove.840B.300d.txt\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/atlas/u/pkalluri/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aeea36-9d69-4811-a56c-8a0930d56587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN LEAVE THESE PARAMETERS AS THE DEFAULTS IF DESIRED\n",
    "# Number of regions to analyze\n",
    "n_regions = 10\n",
    "\n",
    "# How many samples would you like to compare to? (akin to how big of a region)\n",
    "n_neighbors = 100\n",
    "\n",
    "# Token filter. Any center tokens or specific kinds of tokens?\n",
    "# Filter can be: None, top (top tokens only), partial (partial words), of a range a-b, e.g. 10-20 (tokens between the 10th and 20th percentile)\n",
    "filt = '4-5'\n",
    "\n",
    "n_layers = None  # None results in analyzing all layers\n",
    "\n",
    "# Number of GLOVE bands to split and investigate\n",
    "n_bands=10\n",
    "\n",
    "# # Where to save the data, relative to the data directory\n",
    "# output_dir = 'distances'\n",
    "# # Tag to attach to saved data. e.g. to indicate something special about this run.\n",
    "# tag = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be78766c-215e-4463-b1f1-b432d9d6aa5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get tokens and neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672409a6-f5fd-420e-8b30-deaac3111121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read corpus\n",
    "model = MyModel(model_type=model_type)\n",
    "dataset_dir = os.path.abspath(dataset_dir)\n",
    "dataset_toks = [Token(doc,pos,model_type) for doc,pos in pickle.load(open(os.path.join(dataset_dir, refs.toks_fn),'rb'))]\n",
    "dataset_acts = np.load(os.path.join(dataset_dir, refs.acts_fn))\n",
    "words_counts = Counter([tok.word for tok in dataset_toks])\n",
    "layers = select_layers(list(dataset_acts), n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d43b8-48ba-4e6c-9b7d-16b3ca0c6857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter tokens\n",
    "if filt in ['', ' ', None]:\n",
    "    f = lambda tok: True\n",
    "elif filt == 'partial':\n",
    "    f = lambda tok: tok.is_partial\n",
    "elif filt == 'top':\n",
    "    top_n, _ = zip(*types_counts.most_common(n_tokens + 10))\n",
    "    f = lambda tok: tok.type in top_n\n",
    "elif filt[0].isdigit():  # e.g. 10-20: get instances in between and top 10th percentile and 20th percentile\n",
    "    top_bound, bottom_bound = filt.split('-')\n",
    "    top_bound = int(int(top_bound) / 100 * len(words_counts))  # smaller number (nearer to top 1st percentile)\n",
    "    bottom_bound = int(int(bottom_bound) / 100 * len(words_counts))  # larger number (nearer to bottom words)\n",
    "    valid_words, _ = zip(*words_counts.most_common()[top_bound:bottom_bound])\n",
    "    f = lambda tok: tok.type in valid_words\n",
    "# Note: these tokens are all unique - however there may be duplicates of the same type\n",
    "candidate_ids = list(filter(lambda tok_id: f(dataset_toks[tok_id]) and dataset_toks[tok_id].is_valid, range(len(dataset_toks))))\n",
    "toks_ids = random.sample(candidate_ids, n_regions)  # Sampling from the valid corpus\n",
    "print('Words: ', ' '.join([dataset_toks[tok_id].word for tok_id in toks_ids]))\n",
    "# for id_ in toks_ids: print(types_counts[dataset_toks[id_].word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d045502-0701-42b5-96dc-678f20841385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get KNNs\n",
    "\n",
    "# Normally commented out. Overrides earlier set number of layers. Useful for debugging\n",
    "# n_layers = 6\n",
    "# layers = select_layers(list(dataset_acts), n_layers)\n",
    "\n",
    "neighborhoods = pd.DataFrame(columns=[])\n",
    "# dataset_neighbors = {}\n",
    "for layer in layers:\n",
    "    print(f'Analyzing layer {layer}.')\n",
    "    _acts = spherize(dataset_acts[layer])  \n",
    "    # spherizing implicitly changes the distance metric to cosine distance\n",
    "    print('Fitting nearest neighbors model.')\n",
    "    knn_model = FastNearestNeighbors().fit(_acts)\n",
    "    toks_acts = [_acts[tok_id] for tok_id in toks_ids]\n",
    "    del _acts\n",
    "    print('Finding neighbors.')\n",
    "    _, neighborhoods_ids = knn_model.kneighbors(toks_acts, n_neighbors=n_neighbors, return_distance=True)\n",
    "    del knn_model\n",
    "    for tok_id, neighborhood_ids in zip(toks_ids, neighborhoods_ids):\n",
    "        neighborhood = pd.DataFrame(columns=['layer', 'token_id', 'neighbor_rank', 'neighbor_id'])\n",
    "        neighborhood['neighbor_rank'] = range(n_neighbors)\n",
    "        neighborhood['neighbor_id'] = neighborhood_ids\n",
    "        neighborhood['token_id'] = tok_id\n",
    "        neighborhood['layer'] = layer.split('_')[1]\n",
    "        neighborhoods = pd.concat([neighborhoods, neighborhood], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb92dc-7ba6-450f-8d1b-8a73bfc529aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tag neighbors with interesting traits\n",
    "e.g. Is this neighbor the same word? Is it close in GLOVE space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792738b-edb9-4752-9dbe-5304e55b20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag with interesting traits for upcoming visualization\n",
    "neighborhoods['token'] = neighborhoods.token_id.apply(lambda tok_id: dataset_toks[tok_id])\n",
    "neighborhoods['text'] = neighborhoods.token.apply(lambda tok: tok.text)\n",
    "neighborhoods['word'] = neighborhoods.token.apply(lambda tok: tok.word)\n",
    "neighborhoods['neighbor_token'] = neighborhoods.neighbor_id.apply(lambda tok_id: dataset_toks[tok_id])\n",
    "neighborhoods['neighbor_text'] = neighborhoods.neighbor_token.apply(lambda tok: tok.text)\n",
    "neighborhoods['neighbor_word'] = neighborhoods.neighbor_token.apply(lambda tok: tok.word)\n",
    "# neighborhoods['word_count'] = neighborhoods.neighbor_token.apply(lambda tok: words_counts[tok.word])\n",
    "def get_pair_text(pair, k=5):\n",
    "    words1 = pair.text.split(' ')\n",
    "    text1 =  '<br>'.join([' '.join(words1[i:i+k]) for i in range(0,len(words1), k)])\n",
    "    words2 = pair.neighbor_text.split(' ')\n",
    "    text2 =  '<br>'.join([' '.join(words2[i:i+k]) for i in range(0,len(words2), k)]) \n",
    "    return f'{text1} <--> {text2}'\n",
    "neighborhoods['pair'] = neighborhoods.apply(get_pair_text, axis=1)\n",
    "def get_key(pair):\n",
    "    if pair.word == pair.neighbor_word: key = 'Same word'\n",
    "    elif pair.token.doc == pair.neighbor_token.doc: key = 'Same sentence'\n",
    "    elif pair.token.prev == pair.neighbor_token.prev: key = 'Same prev'\n",
    "    else: key = 'Other'\n",
    "    return key\n",
    "neighborhoods['key'] = neighborhoods.apply(get_key, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e0955-1c7e-4985-b044-bb6a02000359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tag with glove distance\n",
    "def get_glove_embs(fp, n_glove_embs=10000):\n",
    "    \"\"\"Load the specified number of glove embeddings.\"\"\"\n",
    "    gloveID_to_word = []\n",
    "    word_to_gloveID = {}\n",
    "    embs = np.array([])\n",
    "    for gloveID, word_emb in enumerate(list(open(glove_file))[:n_glove_embs]):\n",
    "        if gloveID % 1000 == 0: print(f'Processing {gloveID}.')\n",
    "        word, emb = word_emb.split(' ', 1)\n",
    "        gloveID_to_word.append(word)\n",
    "        word_to_gloveID[word] = gloveID\n",
    "        emb = [float(val) for val in emb.split(' ')]\n",
    "        embs = np.array([emb]) if not embs.any() else np.append(embs, [emb], axis=0)\n",
    "    return gloveID_to_word, word_to_gloveID, embs\n",
    "gloveID_to_word, word_to_gloveID, glove_embs = get_glove_embs(glove_file)\n",
    "def get_glove_emb(word):\n",
    "    \"\"\"Get glove embedding of word\"\"\"\n",
    "    return glove_embs[word_to_gloveID[word]]\n",
    "def get_glove_distance(word1, word2, default=0):\n",
    "    \"\"\"\n",
    "    Get glove distance between two words. \n",
    "    If either is not in loaded glove embeddings, return default.\n",
    "    \"\"\"\n",
    "    if word1 in word_to_gloveID and word2 in word_to_gloveID:\n",
    "        return np.abs(np.linalg.norm(get_glove_emb(word1)-get_glove_emb(word2)))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acce9be-2581-4e3a-831d-927f37851194",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods['glove_distance'] = neighborhoods.apply(\n",
    "    lambda pair: get_glove_distance(pair.word, pair.neighbor_word), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51984d-a0d4-4669-a1aa-f13942e602fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag with glove distance band\n",
    "thresholds = [0] + [neighborhoods[(neighborhoods.glove_distance!=0)].glove_distance.quantile(i/n_bands) for i in range(1, n_bands+1)]\n",
    "def get_glove_band(glove_dist):\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        if glove_dist <= threshold: \n",
    "            return i\n",
    "neighborhoods['glove_band'] = neighborhoods.glove_distance.apply(get_glove_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f4690-1b2e-4216-9206-de92aa9d292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN PROGRESS: TAGG BY HOW CLOSE NEIGHBORS ARE IN WORD EMBEDDING DISTANCE OR LAYER 0 DISTANCE\n",
    "# def get_init_distance(tok_id, neighbor_id, data=neighborhoods):\n",
    "#     init_distance = data[(data.layer=='arr_0') & \n",
    "#                          (data.token_id==tok_id) & \n",
    "#                          (data.neighbor_id==neighbor_id)].reset_index().neighbor_rank\n",
    "#     if len(init_distance) != 0:\n",
    "#         return init_distance[0]\n",
    "#     else:\n",
    "#         return 101\n",
    "# neighborhoods['initial_distance'] = neighborhoods.apply(\n",
    "#     lambda pair: get_init_distance(pair.token_id, pair.neighbor_id), axis=1)\n",
    "# def get_pre_distance(pair):\n",
    "#     init_distance = view[(view.layer=='arr_0') & \n",
    "#                          (view.token_id==pair.token_id) & \n",
    "#                          (view.neighbor_id==pair.neighbor_id)].reset_index().neighbor_rank\n",
    "#     if len(init_distance) != 0:\n",
    "#         return init_distance[0]\n",
    "#     else:\n",
    "#         return 101\n",
    "# neighborhoods['initial_distance'] = neighborhoods.apply(get_init_distance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7e3b5d-5f55-4118-8bae-ceee1634ee10",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7406b04-bb63-45cb-b8b0-4d94c26a4d6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple visualization of approaching tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854e696-5735-453b-a759-215b392d630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(neighborhoods, x='layer', y='neighbor_rank', color='key', hover_name='pair', \n",
    "       width=800, height=300, labels=dict(neighbor_rank='', layer='Layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a916c72-0798-4a67-b896-cf4d7f9cf088",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(neighborhoods, x='layer', y='neighbor_rank', color='key', hover_name='pair', \n",
    "               stripmode='overlay', labels=dict(neighbor_rank='', layer='Layer'))\n",
    "fig.update_traces(marker=dict(symbol='circle', size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e555b9-c98b-4159-8110-5d5ac28ea881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you want to ee each word seperately. \n",
    "# This is useful because different words (especially different frequency bands) act very differently.\n",
    "fig = px.strip(neighborhoods, x='layer', y='neighbor_rank', color='key', facet_row='word',\n",
    "       hover_name='pair', width=800, height=1200, labels=dict(neighbor_rank='', layer='Layer'))\n",
    "fig.for_each_annotation(lambda a: a.update(text='\\\"'+a.text.split('=')[1]+'\\\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528432e1-3607-4b89-baf7-fb20c5b9c93b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Heatmap of approaching tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd57cc-4a33-4e65-be79-148a02761917",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_i = {token: i for i, token in enumerate(toks_ids)}\n",
    "neighborhoods['x'] = neighborhoods.apply(lambda row: float(row.layer) + .08 * token_i[row.token_id], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e3a3f-a25c-4973-a1e6-83f4d064ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_view(max_rank=n_neighbors, data=neighborhoods, height=None):\n",
    "    \"\"\"Grab neighbors with rank less than max_rank and show.\n",
    "    This is different than a strip because each column of points corresponds to one point.\n",
    "    This is important because otherwise you see all colors at all ranks without being able to tell why that is.\n",
    "    In other words, this vis combines the above summary and word visualizations.\"\"\"\n",
    "    miniview = data[(data.neighbor_rank < max_rank)]\n",
    "    height = height if height else max_rank\n",
    "    fig = px.scatter(miniview, x='x', y='neighbor_rank', color='key', hover_name='pair',\n",
    "             height=height, template='plotly_white',\n",
    "             category_orders=dict(key=['Same word', 'Other', 'Same sentence']),\n",
    "             opacity=1, labels=dict(neighbor_rank='', x='Layer'),\n",
    "             symbol_sequence=['circle'], range_y=[0,max(max_rank, 100)])\n",
    "    return fig.update_traces(marker=dict(size=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0c580-2e1a-43b1-bbb1-5a377ea5ef3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_view(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa32e5-7001-446a-a7c9-a175b210f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_view(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce11ef9-1cba-4ec5-9ec4-5e9932d946c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_view(n_neighbors, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008c06f-55a7-4fd5-ac7e-1e072b5c9ab6",
   "metadata": {},
   "source": [
    "### Heatmap of which glove distances are approaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d356d-5d7c-4e1d-ae5a-8e3047b020f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_view(max_rank=n_neighbors, data=neighborhoods):\n",
    "    miniview = data[(data.neighbor_rank < max_rank)]\n",
    "    fig = px.scatter(miniview, x='x', y='neighbor_rank', color='glove_distance', hover_name='pair', template='plotly_white',\n",
    "         labels=dict(neighbor_rank='', layerish='Layer', glove_distance='Glove distance', glove_distance_v2='Glove distance'), \n",
    "         color_continuous_scale=px.colors.sequential.Rainbow[::-1], symbol_sequence=['circle'], range_y=[0,max(max_rank,100)])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17c8f7-7133-431f-a7c0-df2345b74d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_view(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b661f31-19f6-4ddb-8c3a-e6b1bc00aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_view(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecf5c3-47e1-4385-8ab1-ddead7a2bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_view(n_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa98cf6-d25c-4d93-945d-e36aa9675974",
   "metadata": {},
   "source": [
    "### Heatmap of which glove bands are approaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d7616-68d9-4771-a25a-84c4a2c7d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_view(max_rank=n_neighbors, data=neighborhoods):\n",
    "    miniview = data[(data.neighbor_rank < max_rank)]\n",
    "    miniview['glove_band_name'] = miniview.glove_band.astype(str)\n",
    "    fig = px.scatter(miniview, x='x', y='neighbor_rank', color='glove_band_name', hover_name='pair',\n",
    "         template='plotly_white',  # color_discrete_sequence=['red', 'yellow', 'limegreen', 'blue'] + ['black']*10, \n",
    "         category_orders=dict(glove_band_name=[str(i) for i in range(n_bands+1)]),\n",
    "         labels=dict(neighbor_rank='', layerish='Layer', glove_band='Glove band', glove_band_name='Glove band'), \n",
    "         hover_data=['glove_distance'], range_y=[0,max(max_rank,100)])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18244c8-fabb-473c-a18b-146c3573fad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "band_view(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f475cdd9-2226-4b6c-8732-65d5cdc9c5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "band_view(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c84223-cfaf-4bd2-a702-46b9fea5f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_view(n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4be36-6348-46e0-b2ba-3ca8c5dfea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "data = view\n",
    "counts = pd.DataFrame()\n",
    "tok_region_sizes = {tok: len(data[(data.token==tok) & (data.layer==layer) & (data.key=='Same word')]) for tok in data.token.unique()}\n",
    "for layer in layers:\n",
    "    print('Layer', layer)\n",
    "    for key in data.key.unique():\n",
    "        _counts = pd.Series()\n",
    "        _counts['layer'] = layer\n",
    "        _counts['word'] = ''\n",
    "        _counts['key'] = key\n",
    "        _counts['count'] = len(data[(data.layer == layer) & (data.key == key)])\n",
    "        counts = counts.append(_counts, ignore_index=True)\n",
    "        for tok in data.token.unique():\n",
    "            _counts = pd.Series()\n",
    "            _counts['layer'] = layer\n",
    "            _counts['text'] = f'{tok.word}-{len(tok.doc)}'\n",
    "            _counts['key'] = key\n",
    "            count = len(data[(view.layer == layer) & \n",
    "                             (view.key == key) & \n",
    "                             (view.token == tok) & \n",
    "                             (view.neighbor_rank < tok_region_sizes[tok])])\n",
    "            _counts['count'] = count\n",
    "            _counts['fraction'] = count / tok_region_sizes[tok]\n",
    "            counts = counts.append(_counts, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbf2ec-0ceb-45f9-ba77-b40ff54ede47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig = px.bar(counts[counts.word != ''], x='layer', y='count', color='key', height=900,\n",
    "#        facet_row='text')\n",
    "fig = px.bar(counts[counts.word != ''], x='text', y='fraction', color='key', height=400,\n",
    "       facet_col='layer', category_orders=dict(key=['Same word']))\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579514df-1d4e-44fb-b5eb-e7b6c27776c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(counts[counts.word != ''], x='layer', y='fraction', \n",
    "       color='key', height=400, category_orders=dict(key=['Same word']),\n",
    "       width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b720c-3c90-43ec-ba57-086d75f1c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "data = view[(view.neighbor_rank < 10000)]\n",
    "counts = pd.DataFrame()\n",
    "tok_region_sizes = {tok: len(data[(data.token==tok) & (data.layer==layer) & (data.key=='Same word')]) for tok in data.token.unique()}\n",
    "for layer in layers:\n",
    "    print('Layer', layer)\n",
    "    for glove_band in data.glove_band.unique():\n",
    "        _counts = pd.Series()\n",
    "        _counts['layer'] = layer\n",
    "        _counts['word'] = ''\n",
    "        _counts['glove_band'] = glove_band\n",
    "        _counts['count'] = len(data[(data.layer == layer) & (data.glove_band == glove_band)])\n",
    "        counts = counts.append(_counts, ignore_index=True)\n",
    "        for tok in data.token.unique():\n",
    "            _counts = pd.Series()\n",
    "            _counts['layer'] = layer\n",
    "            _counts['text'] = f'{tok.word}-{len(tok.doc)}'\n",
    "            _counts['glove_band'] = glove_band\n",
    "            points = data[(view.layer == layer) & \n",
    "                             (view.glove_band == glove_band) & \n",
    "                             (view.token == tok)]\n",
    "            _counts['count'] = len(points)\n",
    "            _counts['fraction'] = len(points[(view.neighbor_rank < tok_region_sizes[tok])]) / tok_region_sizes[tok]\n",
    "            counts = counts.append(_counts, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0671fba1-8172-4c59-a495-9bbd4329555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(counts[(counts.word != '')], x='layer', y='count', \n",
    "       color='glove_band', height=400, barmode='group',\n",
    "       width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37d1a0-9872-4306-9a9e-4c6f0788660f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Distribution of approaching glove bands\n",
    "In the nearest 10 tokens, what is the distribution of glove bands?\n",
    "What about in the nearest 100 tokens? So on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03593701-88a1-4226-9d82-78c6553eeb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_band = view[(view.neighbor_rank < 5) & (view.glove_distance != 0)]\n",
    "fig = px.histogram(distance_band, x='glove_distance', facet_col='layer', barmode='group', color='glove_band',\n",
    "             category_orders=dict(layer=layers, glove_band=list(range(n_bands))),\n",
    "             labels=dict(glove_band='Glove distance', glove_distance='')).for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975316b6-73e5-4c97-b594-f58b273702f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_band = view[(view.neighbor_rank < 10) & (view.glove_distance != 0)]\n",
    "fig = px.histogram(distance_band, x='glove_distance', facet_col='layer', barmode='group', color='glove_band',\n",
    "             category_orders=dict(layer=layers, glove_band=list(range(n_bands))),\n",
    "             labels=dict(glove_band='Glove distance', glove_distance='')).for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee16b9a-7a75-461c-824b-f4aa3d8d2b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_band = view[(view.neighbor_rank < 100) & (view.glove_distance != 0)]\n",
    "fig = px.histogram(distance_band, x='glove_distance', facet_col='layer', barmode='group', color='glove_band',\n",
    "             category_orders=dict(layer=layers, glove_band=list(range(n_bands))),\n",
    "             labels=dict(glove_band='Glove distance', glove_distance='')).for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c72ef6f-80b8-41eb-9005-9d5291cda3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_band = view[(view.neighbor_rank < 1000) & (view.glove_distance != 0)]\n",
    "px.histogram(distance_band, x='glove_distance', facet_col='layer', barmode='group', color='glove_band',\n",
    "             category_orders=dict(layer=layers, glove_band=list(range(n_bands))),\n",
    "             labels=dict(glove_band='Glove distance', glove_distance='')).for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4baf6-776d-4963-9bd3-57654df43527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_band = view[(view.neighbor_rank < 10000) & (view.glove_distance != 0)]\n",
    "px.histogram(distance_band, x='glove_distance', facet_col='layer', barmode='group', color='glove_band',\n",
    "             category_orders=dict(layer=layers, glove_band=list(range(n_bands))),\n",
    "             labels=dict(glove_band='Glove distance', glove_distance='')).for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a585489-75f9-42dc-ae53-70a88c2cc41f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_big_bands = 4\n",
    "neighborhoods['glove_group'] = (neighborhoods.glove_band*n_big_bands/n_bands).astype(int)\n",
    "\n",
    "def distribution_of_bands_view(max_rank=n_neighbors, data=neighborhoods):\n",
    "    model_band = data[(data.neighbor_rank < max_rank) & (data.glove_group >= 0)]\n",
    "    return px.histogram(model_band, x='layer', color='glove_group', \n",
    "            category_orders=dict(layer=layers, glove_group=list(range(n_big_bands))),\n",
    "              color_discrete_sequence=color_sequences.Plotly, labels=dict(glove_group='Glove band', layer='Layer')\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231cc8a-9d16-4ac8-a8c3-d3d429c833d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_of_bands_view(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560bb39-9063-4f64-b3bc-001e206039b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_of_bands_view(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3f323-eae4-44b3-af90-13a92117e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_of_bands_view(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd465c-fd67-4ebc-a18e-76be5222bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_of_bands_view(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66fa47-5cce-4be1-9f55-4af49a52766d",
   "metadata": {},
   "source": [
    "### What happens to each glove band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe20be-ddf6-4f8c-84a0-42102ac68698",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(neighborhoods[(neighborhoods.neighbor_rank<10)], x='layer', y='neighbor_rank', facet_col='glove_group', facet_col_wrap=2,\n",
    "       color='glove_group', \n",
    "    category_orders=dict(layer=layers,glove_group=list(range(len(neighborhoods.glove_group.unique())))), \n",
    "    height=1000,\n",
    "    labels=dict(neighbor_rank='Distance (rank)', glove_group='Glove band'))\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.replace('=',' ')))\n",
    "# fig.update_xaxes(nticks=len(layers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
