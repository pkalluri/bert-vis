{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09902bc2-2c4b-40f9-aeec-cf6ce39fff8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyze distances from tokens to regions of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f8be8-1390-410b-8afa-16c3729360bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Notebook is working.')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/atlas/u/pkalluri/.cache'\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../..'))  # distances --> vis-wiki --> analysis-and-vis --> src\n",
    "# This form of import is reasonable research practice because many directories may want to use the same utils,\n",
    "# but note that this is a bad practice for publishing packages because directories should be modular, with all utils inside them.\n",
    "# Consider changing before publicly publishing code.\n",
    "# load\n",
    "from utils import references as refs\n",
    "import pickle\n",
    "import numpy as np\n",
    "from utils.Token import Token\n",
    "from utils.misc_util import select_layers\n",
    "from collections import Counter\n",
    "# process\n",
    "import random\n",
    "from utils.ModelType import ModelType, get_generic, berts, gpts\n",
    "from utils.MyModel import MyModel\n",
    "# calculate distances\n",
    "from utils.acts_util import get_angles, get_euclidean_distances\n",
    "import pandas as pd\n",
    "# vis\n",
    "import plotly.express as px\n",
    "from utils.plotly_util import get_error_bands, combine_figs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b5556-e723-4004-9df4-14e14d5c7278",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb5105-3d0c-42a9-9132-00ed6e882be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and data params\n",
    "model_type = ModelType.bert_base_cased\n",
    "dataset_dir = f\"/atlas/u/pkalluri/bert-vis/big-data/{get_generic(model_type)}/{model_type.value}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04d23c-e97a-40f4-9969-40f4efa5d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other params\n",
    "\n",
    "# Number of center tokens to analyze\n",
    "n_tokens = 10\n",
    "\n",
    "# Any center tokens, or specific kinds of tokens? e.g. Do you want to know if frequent tokens get farther from their type balls?\n",
    "filt = ''  # None or 'frequent.__' or 'partial' or 'top'\n",
    "\n",
    "# How many samples would you like to compare to? i.e. How big would you like the subcorpora and custom corpora to be? \n",
    "n_samples = 10\n",
    "\n",
    "n_layers = None  # None results in analyzing all layers\n",
    "\n",
    "# Where to save the data\n",
    "output_dir = 'distances'\n",
    "\n",
    "# Tag to attach to saved data. e.g. to indicate something special about this run.\n",
    "tag = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e37f52-369f-4e84-a6b4-4b07ddde4666",
   "metadata": {},
   "source": [
    "## Set up setup, data model, and choose center tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f699a-436f-4535-bce7-9418953e2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(model_type=model_type)\n",
    "dataset_dir = os.path.abspath(dataset_dir)\n",
    "dataset_toks = [Token(doc,pos,model_type) for doc,pos in pickle.load(open(os.path.join(dataset_dir, refs.toks_fn),'rb'))]\n",
    "types_counts = Counter([tok.type for tok in dataset_toks])\n",
    "dataset_acts = np.load(os.path.join(dataset_dir, refs.acts_fn))\n",
    "layers = select_layers(list(dataset_acts), n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5198b43-84d2-4e57-a18b-7adc62198c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict, constructing a smaller corpus\n",
    "if filt in ['', ' ', None]:\n",
    "    f = lambda tok: not tok.is_special\n",
    "elif filt=='partial':\n",
    "    f = lambda tok: tok.is_partial\n",
    "elif filt.startswith('frequent'): # e.g. get elements arund the 90th percentile\n",
    "    start = float(filt.split('frequent')[1]) * len(types_counts)\n",
    "    valid_types = types_counts.most_common()[::-1][start:start+n_samples]\n",
    "    f = lambda tok: tok.type in valid_types and not tok.is_special\n",
    "elif filt=='top':\n",
    "    top_n, _ = zip(*types_counts.most_common(n_tokens+2))\n",
    "    f = lambda tok: tok.type in top_n and not tok.is_special\n",
    "# Note: these tokens are all unique - however there may be duplicates of the same type\n",
    "candidate_ids = list(filter(lambda tok_id: f(dataset_toks[tok_id]), range(len(dataset_toks))))\n",
    "# print(candidate_ids)\n",
    "tok_ids = random.sample(candidate_ids, n_tokens) # Sampling from the valid corpus\n",
    "print('Types: ', ' '.join([dataset_toks[tok_id].type for tok_id in tok_ids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6412b1a-6ab5-461f-9141-57562b291ea6",
   "metadata": {},
   "source": [
    "## Choose subcorpora to compare to\n",
    "Choose subcorpora of entire dataset, to see how far away a token is from this subcorpus.\n",
    "\n",
    "e.g. you might interested in how far away tokens are from their type ball (tokens of same type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da3cd1-0abb-4725-a222-4f12190bf1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define\n",
    "filters = {}\n",
    "filters['Random tokens'] = lambda main_tok, candidate_tok: not candidate_tok.is_edge\n",
    "filters[f'Same type, in naturally occurring contexts'] = lambda tok1, tok2: tok1 != tok2 and tok1.same_type(tok2)\n",
    "filters[f'Same successor, in naturally occurring contexts'] = (\n",
    "    lambda tok1, tok2: tok1.same_next(tok2) and not tok1.same_type(tok2) and not tok2.is_edge)\n",
    "filters[f'Same bigram, natural'] = (\n",
    "    lambda tok1, tok2: tok1.same_type(tok2) and tok1.same_next(tok2) and not tok2.is_edge)\n",
    "filters[f'Partner in bigram, natural'] = (\n",
    "    lambda tok1, tok2: tok2.prev == tok1.type and tok1.next == tok2.type and not tok2.is_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d0260-e35f-4ab5-9d9e-02b07ee09b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply each filter, constructing the subcorpora of interest\n",
    "subcorpora = {filter_:{tok_id:[] for tok_id in tok_ids}  for filter_ in filters}\n",
    "# e.g. a subcorpus is filter (\"same type\") as this token (\"...caught...\")\n",
    "for candidate_tok_id in range(len(dataset_toks)):  # scan through dataset for relevant tokens\n",
    "    candidate_tok = dataset_toks[candidate_tok_id]\n",
    "    for tok_id in tok_ids:\n",
    "        tok = dataset_toks[tok_id]\n",
    "        for filter_, f in filters.items():\n",
    "            subcorpus = subcorpora[filter_][tok_id]\n",
    "            if len(subcorpus) < n_samples and f(tok, candidate_tok):  \n",
    "                # continue gathering points relevant to this subcorpus\n",
    "                subcorpus.append(candidate_tok_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c6c12-e85c-450a-9418-27834aeea101",
   "metadata": {},
   "source": [
    "## Create custom corpora to compare to \n",
    "e.g. You may be interested in comparing this token to the same type dropped into random contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babbfd1-54c3-4db4-99c0-50f70d9bba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define\n",
    "recipes = {}\n",
    "random_toks = random.choices(\n",
    "    list(filter(lambda tok: not tok.is_special and not tok.is_edge, dataset_toks)), \n",
    "    k=n_samples)\n",
    "recipes[f'Same type, dropped into random contexts'] = lambda tok: tok.in_contexts(random_toks)\n",
    "recipes[f'Same successor, dropped into random contexts'] = (\n",
    "    lambda tok: [rand_tok.replace(tok.next, pos=rand_tok.pos+1) for rand_tok in random_toks])\n",
    "recipes[f'Same bigram, dropped into random contexts'] = (\n",
    "    lambda tok: [rand_tok.replace(tok.type).replace(tok.next, pos=rand_tok.pos+1) for rand_tok in random_toks])\n",
    "random_types = random.choices(\n",
    "    list(filter(lambda type_: not Token.is_type_special(type_, model_type), types_counts.keys())), \n",
    "    k=n_samples)\n",
    "recipes[f'Random types, dropped into same context'] = lambda tok: tok.with_types(random_types)\n",
    "def random_types_in_natural_contexts(tok):\n",
    "    natural_toks = random.choices(\n",
    "        list(filter(lambda candidate_tok: candidate_tok.same_type(tok) and not candidate_tok.is_edge, \n",
    "                    dataset_toks)), k=n_samples)\n",
    "    return [natural_tok.replace(random_type) for natural_tok, random_type in zip(natural_toks, random_types)]\n",
    "recipes[f'Random types, dropped in naturally occurring context'] = random_types_in_natural_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c24e7-d9ed-4010-b845-50e11589c2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply recacts_utiles, to construct the custom corpora to compare to\n",
    "custom_corpora = {}\n",
    "print('Constructing corpora...')\n",
    "for recipe, f in recipes.items():\n",
    "    print(recipe)\n",
    "    custom_corpora[recipe] = {tok_id: f(dataset_toks[tok_id]) for tok_id in tok_ids}  # a custom corpus is e.g. \"tokens in doc\" of this specific doc\n",
    "print('\\nPulling activations...')\n",
    "custom_corpora_acts = {}\n",
    "for recipe in recipes:\n",
    "    print(recipe)\n",
    "    custom_corpora_acts[recipe] = {tok_id: model.get_toks_acts(custom_toks) for tok_id, custom_toks in custom_corpora[recipe].items()}        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0633f4-7c44-4231-b907-8643d7bf400c",
   "metadata": {},
   "source": [
    "## Calculating distances to tokens of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad93f7-2a15-4da3-9d97-528e82244517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "get_distances = get_angles # metric\n",
    "n_layers=None\n",
    "layers = select_layers(list(dataset_acts), n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30741ac-65d9-4ddb-859c-9de657313a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate\n",
    "distances = pd.DataFrame()\n",
    "dim = dataset_acts[layers[0]][0].shape[0]\n",
    "for layer in layers:\n",
    "    print(layer)\n",
    "    _dataset_acts = dataset_acts[layer]\n",
    "    for tok_id in tok_ids:\n",
    "        tok = dataset_toks[tok_id]\n",
    "        _tok_act = _dataset_acts[tok_id]\n",
    "        # filters on dataset\n",
    "        for filter_ in filters:\n",
    "            subcorpus_ids = subcorpora[filter_][tok_id]\n",
    "            subcorpus_distances = {\n",
    "                'other token': [dataset_toks[id_] for id_ in subcorpus_ids],\n",
    "                'distance': get_distances(_dataset_acts[subcorpus_ids], _tok_act),\n",
    "                'layer': [layer,] * len(subcorpus_ids),\n",
    "                'key': [filter_,] * len(subcorpus_ids),\n",
    "                'main token': [tok,] * len(subcorpus_ids)}\n",
    "            distances = pd.concat([distances, pd.DataFrame(subcorpus_distances)], ignore_index=True)\n",
    "            del subcorpus_distances\n",
    "        # custom corpora\n",
    "        for recipe in recipes:\n",
    "            custom_corpus = custom_corpora[recipe][tok_id]\n",
    "            _custom_corpus_acts = custom_corpora_acts[recipe][tok_id][layer]\n",
    "            custom_corpus_distances = {\n",
    "                'other token': custom_corpus,\n",
    "                'distance': get_distances(_custom_corpus_acts, _tok_act),\n",
    "                'layer': [layer,] * len(custom_corpus),\n",
    "                'key': [recipe,] * len(custom_corpus),\n",
    "                'main token': [tok,] * len(custom_corpus)}\n",
    "            distances = pd.concat([distances, pd.DataFrame(custom_corpus_distances)], ignore_index=True)\n",
    "            del custom_corpus_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef71832-86a6-4819-8eed-de5b777b5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "def tok_text(tok, k=5):\n",
    "    words = (tok.text if type(tok) is not str else \"\").split(' ')\n",
    "    return '<br>'.join([' '.join(words[i:i+k]) for i in range(0,len(words), k)])\n",
    "distances['main token text'] = distances['main token'].apply(tok_text)\n",
    "distances['other token text'] = distances['other token'].apply(tok_text)\n",
    "distances['layer'] = distances['layer'].apply(lambda arr: arr.split('_')[1] if len(arr.split('_'))>1 else arr)\n",
    "# Save\n",
    "if not n_layers: n_layers = len(list(dataset_acts))\n",
    "out_dir_path = os.path.join(dataset_dir, output_dir)\n",
    "if not os.path.exists(out_dir_path):\n",
    "    os.mkdir(out_dir_path)\n",
    "save_path = os.path.join(out_dir_path,\n",
    "    f'types_vs_contexts_{n_tokens}{filt}tokens_to_{n_samples}neighbors_across_{n_layers}layers{tag}.csv')\n",
    "distances.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf4e4a-c72c-4cb4-957d-a15213da52c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5124e7c-335b-4d23-a110-117454aaab21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f3e04-365b-41e6-9917-7a7a1bf9d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "data_path = '' # Path to data. If empty, defaults to above created save path.\n",
    "bands = True  # Add quartile bands around the lines\n",
    "keys = [\n",
    "#     'Random tokens', \n",
    "    'Same type, in naturally occurring contexts',\n",
    "#     'Same predecessor, in naturally occurring contexts',\n",
    "#     'Same successor, in naturally occurring contexts',\n",
    "    'Same type, dropped into random contexts',\n",
    "    'Random types, dropped into same context',\n",
    "#     'Random types, dropped in naturally occurring context'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840a10c-9349-478c-98c3-8c3c877350c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layers = distances['layer'].unique().tolist()\n",
    "if not data_path: data_path = save_path\n",
    "# distances - pull from csv\n",
    "for layer in layers:\n",
    "    median_rand = distances[(distances.key=='Random tokens') & (distances.layer==layer)]['distance'].median()\n",
    "    distances.loc[distances.layer==layer, 'normalized distance'] = distances[distances.layer==layer]['distance']/median_rand\n",
    "median = pd.concat([pd.DataFrame(dict(layer=layers)), \n",
    "                   pd.DataFrame({key: [distances[(distances.layer==layer) & (distances.key==key)]['normalized distance'].quantile(.5) for layer in layers] for key in keys})],\n",
    "                   axis=1)\n",
    "q1 = pd.concat([pd.DataFrame(dict(layer=layers)), \n",
    "                   pd.DataFrame({key: [distances[(distances.layer==layer) & (distances.key==key)]['normalized distance'].quantile(.25) for layer in layers] for key in keys})],\n",
    "                   axis=1)\n",
    "q3 = pd.concat([pd.DataFrame(dict(layer=layers)), \n",
    "                   pd.DataFrame({key: [distances[(distances.layer==layer) & (distances.key==key)]['normalized distance'].quantile(.75) for layer in layers] for key in keys})],\n",
    "                   axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8f0c1-604a-4196-b54d-32f1b66535d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Line graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7958c6b-2624-4d4f-a109-d024f956d810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title = ''\n",
    "lines = px.line(median, x='layer', y=keys, width=900, height=500, labels={\"variable\": \"\"}, title=title).update_traces(line=dict(width=3))\n",
    "bands = get_error_bands(keys, q1,q3, layers)\n",
    "\n",
    "# Tweaks\n",
    "upper_bound = px.line(x=layers, y=[1,]*len(layers), color_discrete_sequence=['white',])\n",
    "fig = combine_figs([lines, bands, upper_bound])\n",
    "fig.update_yaxes(title='Distance', range=[0, 1.1], showticklabels=False, showgrid=False, nticks=2)\n",
    "fig.update_xaxes(title=f'Layers in {get_generic(model_type).upper()}', showticklabels=True, tickvals=layers)\n",
    "fig.update_layout(title_x=0.1, title_y=.85)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab8bd4-9ebe-4676-bfaa-7b273738416e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split=False\n",
    "n_rows = len(distances['main token'].unique()) if split else 1\n",
    "facet_rows = 'main token' if split else None\n",
    "fig2 = px.box(distances[(distances.key.isin(keys))], x='layer', y='normalized distance', color='key', \n",
    "               title=title, hover_name='other token text', hover_data={col: False for col in ['layer', 'key', 'normalized distance']}, \n",
    "               width=800, height=n_rows*500,\n",
    "              facet_row=facet_rows,\n",
    "                boxmode='overlay',\n",
    "              notched=False,\n",
    "                points=False,\n",
    "              )\n",
    "upper_bound = px.line(x=layers, y=[1,]*len(layers), color_discrete_sequence=['white',])\n",
    "fig2 = combine_figs([fig2, upper_bound])\n",
    "fig2.update_yaxes(title='Distance', range=[0,1.1], showticklabels=False, showgrid=False, nticks=2)\n",
    "fig2.update_xaxes(title=f'Layers in {model_name.upper()}', showticklabels=True, tickvals=layers)\n",
    "fig2.update_layout(title_x=0.1, title_y=.85)\n",
    "fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb449dbf-52e2-4437-b623-3170ecb5b444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
